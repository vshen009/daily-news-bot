# 数据去重优化需求文档

## 📋 文档信息

- **创建时间**：2026-01-26
- **最后更新**：2026-01-26（根据专家评审意见更新）
- **项目**：定制每日财经新闻
- **版本**：v2.1（基于v2.0的改进版）
- **状态**：待实施
- **评审人**：外部技术专家
- **改进方案**：方案B（P0 + P1级别问题修复）

---

## 🎯 需求背景

### 当前问题

**现象**：
- 抓取了 156 条新闻
- 数据库只保存了 28 条
- 大量新闻被标记为"缓存命中"，丢失了数据

**根本原因**：

1. **新闻源配置重复**
   ```yaml
   # sources.yaml
   - 彭博社（亚太板块）    ← 抓取新闻
   - 彭博社（美欧板块）    ← 再次抓取相同新闻
   ```

2. **数据库唯一约束冲突**
   ```sql
   CONSTRAINT unique_title UNIQUE (title, title_original)
   ```
   - 同一条英文新闻出现在两个板块
   - 第二次保存时因 `title_original` 重复被跳过

3. **去重逻辑过于严格**
   ```python
   def article_exists(self, title_original):
       # 只要 title_original 存在就认为是重复
       query = "SELECT id FROM news_articles WHERE title_original = ?"
   ```

**实际影响**：
- 亚太板块和美欧板块共享相同的5个国际新闻源
- 导致 156 条抓取 → 28 条保存（丢失 82% 的数据）

---

## ✨ 解决方案

### 核心思路

**分两阶段处理**：

```
阶段1：原始数据抓取（已完成 ✅）
  ├─ 抓取所有新闻源
  ├─ 不翻译、不去重
  └─ 保存到临时表 raw_articles

阶段2：数据处理和生成（待实施）
  ├─ 从临时表读取数据
  ├─ 按板块分组去重
  ├─ 筛选重要新闻
  ├─ 翻译英文内容
  ├─ 生成 AI 评论
  └─ 保存到正式表 + 生成 HTML
```

---

## 📊 数据库设计

### 表结构对比

#### 1. 临时表 `raw_articles`（已创建 ✅）

```sql
CREATE TABLE raw_articles (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    title TEXT,                    -- 翻译后的标题（初始为空）
    title_original TEXT,           -- 原始标题（英文）
    content TEXT,                  -- 翻译后的内容（初始为空）
    content_original TEXT,         -- 原始内容（英文）
    source TEXT NOT NULL,          -- 来源（中文名）
    source_original TEXT,          -- 来源（英文名）
    url TEXT,                      -- 文章链接
    category TEXT NOT NULL,        -- 板块分类
    language TEXT NOT NULL,        -- 语言（zh/en）
    publish_time TIMESTAMP,        -- 发布时间
    crawl_time TIMESTAMP NOT NULL, -- 抓取时间
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    -- ❌ 没有唯一约束！保留所有数据
);

-- 索引
CREATE INDEX idx_raw_category ON raw_articles(category);
CREATE INDEX idx_raw_source ON raw_articles(source);
CREATE INDEX idx_raw_crawl_time ON raw_articles(crawl_time DESC);
```

**特点**：
- ✅ 保存所有抓取的原始数据
- ✅ 无去重约束
- ✅ 每次清空后重新抓取

---

#### 2. 正式表 `news_articles`（已存在）

```sql
CREATE TABLE news_articles (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    title TEXT NOT NULL,           -- 翻译后的标题
    title_original TEXT,           -- 原始标题（英文）
    content TEXT,                  -- 翻译后的内容
    content_original TEXT,         -- 原始内容（英文）
    source TEXT NOT NULL,          -- 来源
    source_original TEXT,          -- 来源（英文名）
    url TEXT,                      -- 文章链接
    category TEXT NOT NULL,        -- 板块分类
    language TEXT NOT NULL,        -- 语言
    publish_time TIMESTAMP NOT NULL, -- 发布时间
    crawl_time TIMESTAMP NOT NULL,  -- 抓取时间
    tags TEXT,                      -- 标签
    ai_comment TEXT,                -- AI 评论
    translated BOOLEAN DEFAULT 0,   -- 是否已翻译
    translation_method TEXT,        -- 翻译方法
    featured BOOLEAN DEFAULT 0,     -- 是否精选
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT unique_title UNIQUE (title, title_original)
);

-- 索引
CREATE INDEX idx_publish_time ON news_articles(publish_time DESC);
CREATE INDEX idx_category ON news_articles(category);
CREATE INDEX idx_title_original ON news_articles(title_original);
```

**特点**：
- ✅ 有唯一约束（同一条新闻只保留一份）
- ✅ 保存翻译和 AI 评论
- ✅ 最终生成 HTML 的数据源

---

## 🔄 数据处理流程

### 阶段1：原始数据抓取（已实现 ✅）

**文件**：`test_raw_fetch.py`

**流程**：
```python
1. 初始化数据库（创建 raw_articles 表）
2. 清空临时表（避免旧数据干扰）
3. 抓取所有新闻源（156条）
4. 保存到 raw_articles（无去重）
5. 显示统计信息
```

**结果**：
```
✅ 156 条全部保存到临时表
✅ 按板块分布：
   - asia_pacific: 68 条
   - us_europe: 68 条
   - domestic: 20 条
```

---

### 阶段2：数据处理和生成（待实施 ⏳）

#### 2.1 从临时表读取数据

```python
def load_raw_articles():
    """从临时表读取所有数据"""
    raw_articles = db_manager.get_raw_articles()
    # 返回格式：List[dict]
    return raw_articles
```

**示例数据**：
```python
{
    'id': 1,
    'title': None,  # 英文新闻初始为空
    'title_original': 'Asia-Pacific stocks trade mixed...',
    'source': 'CNBC',
    'category': 'asia_pacific',
    'language': 'en',
    'crawl_time': '2026-01-26 18:07:12'
}
```

---

#### 2.2 按板块分组

```python
def group_by_category(raw_articles):
    """按板块分组"""
    grouped = {
        'asia_pacific': [],
        'us_europe': [],
        'domestic': []
    }

    for article in raw_articles:
        category = article['category']
        if category in grouped:
            grouped[category].append(article)

    return grouped
```

**结果**：
```python
{
    'asia_pacific': [68条],
    'us_europe': [68条],
    'domestic': [20条]
}
```

---

#### 2.2.1 时间字段处理（publish_time保底逻辑）⭐

---

##### 📋 问题：RSS源时间格式混乱

**现象**：
```
很多RSS源提供的发布时间格式极其混乱：
- 时区不同：UTC, EST, PST, 北京时间...
- 格式不一：RFC 3339, ISO 8601, 自定义格式...
- 甚至缺失：有的RSS源根本不提供publish_time
```

**影响**：
```python
# 如果publish_time解析失败，新闻会在时效性过滤阶段消失
article = {
    'title': '重要新闻',
    'publish_time': None,  # ❌ 缺失
    'crawl_time': '2026-01-26 18:00:00'
}

# 24小时过滤
if article['publish_time'] < start_time:
    return None  # ❌ 新闻被错误过滤掉
```

---

##### 💡 解决方案：保底逻辑

**核心思路**：
```
如果publish_time存在 → 使用publish_time
如果publish_time缺失 → 使用crawl_time代替
```

**优势**：
- ✅ 不会因为时间缺失而丢失新闻
- ✅ crawl_time总是可靠的（我们控制的）
- ✅ 记录日志便于后续分析

---

##### 💻 实现逻辑（伪代码）

```python
def get_effective_publish_time(article):
    """
    获取有效的发布时间（v2.0新增 ⭐）

    保底逻辑：
    1. 优先使用publish_time
    2. 如果publish_time为空或无效，使用crawl_time
    3. 记录日志便于后续分析

    返回：
    - datetime对象：有效的发布时间
    """
    # 尝试解析publish_time
    publish_time = article.get('publish_time')

    if publish_time:
        try:
            # 解析时间戳
            parsed_time = parse_timestamp(publish_time)
            logger.debug(f"使用publish_time: {parsed_time}")
            return parsed_time
        except Exception as e:
            logger.warning(f"publish_time解析失败: {publish_time}, 错误: {e}")

    # 保底：使用crawl_time
    crawl_time = article.get('crawl_time')
    if crawl_time:
        logger.info(f"⚠️ publish_time缺失，使用crawl_time: {article['title'][:50]}")
        return parse_timestamp(crawl_time)

    # 极端情况：两者都缺失
    logger.error(f"❌ 时间字段完全缺失: {article['title']}")
    return datetime.now()  # 最后的保底


def parse_timestamp(time_str):
    """
    解析多种时间格式（v2.0新增 ⭐）

    支持的格式：
    - RFC 3339: "2026-01-26T18:00:00Z"
    - ISO 8601: "2026-01-26T18:00:00+08:00"
    - 常见格式: "2026-01-26 18:00:00"
    """
    formats = [
        "%Y-%m-%dT%H:%M:%SZ",           # RFC 3339 (UTC)
        "%Y-%m-%dT%H:%M:%S%z",          # ISO 8601 (时区)
        "%Y-%m-%dT%H:%M:%S",            # ISO 8601 (无时区)
        "%Y-%m-%d %H:%M:%S",            # 标准格式
        "%a, %d %b %Y %H:%M:%S %z",     # RSS格式 (RFC 2822)
    ]

    for fmt in formats:
        try:
            return datetime.strptime(time_str, fmt)
        except ValueError:
            continue

    # 所有格式都失败
    raise ValueError(f"无法解析时间格式: {time_str}")


def normalize_to_utc(time_obj):
    """
    归一化到UTC时区（v2.0新增 ⭐）

    为什么需要：
    - 不同RSS源使用不同时区
    - 需要统一到UTC进行比较

    返回：
    - datetime对象（UTC时区）
    """
    if time_obj.tzinfo is None:
        # 无时区信息，假设为UTC
        return time_obj.replace(tzinfo=timezone.utc)

    # 有时区信息，转换为UTC
    return time_obj.astimezone(timezone.utc)
```

---

##### 📊 实际使用示例

```python
# 示例1：正常情况
article1 = {
    'title': '美联储宣布降息',
    'publish_time': '2026-01-26T10:00:00Z',  # ✅ 正常
    'crawl_time': '2026-01-26 18:00:00'
}
effective_time = get_effective_publish_time(article1)
# → 2026-01-26 10:00:00 UTC ✅

# 示例2：publish_time缺失
article2 = {
    'title': '某RSS源未提供时间',
    'publish_time': None,  # ❌ 缺失
    'crawl_time': '2026-01-26 18:00:00'
}
effective_time = get_effective_publish_time(article2)
# → 2026-01-26 18:00:00 UTC ✅
# 日志：⚠️ publish_time缺失，使用crawl_time

# 示例3：publish_time格式错误
article3 = {
    'title': '某RSS源时间格式异常',
    'publish_time': 'Invalid Date',  # ❌ 格式错误
    'crawl_time': '2026-01-26 18:00:00'
}
effective_time = get_effective_publish_time(article3)
# → 2026-01-26 18:00:00 UTC ✅
# 日志：⚠️ publish_time解析失败: Invalid Date
```

---

##### ✅ 总结

**时间字段处理的核心逻辑**：

1. ✅ **优先使用publish_time**
   - 如果RSS源提供了时间，优先使用
   - 支持多种时间格式解析

2. ✅ **保底使用crawl_time**
   - 如果publish_time缺失或解析失败
   - 使用我们控制的crawl_time
   - 确保不会因时间问题丢失新闻

3. ✅ **统一时区**
   - 所有时间归一化到UTC
   - 避免时区混淆

4. ✅ **详细日志**
   - 记录所有保底情况
   - 便于后续分析RSS源质量

**状态**：✅ 逻辑已确认，v2.0必须实施

---

#### 2.3 按板块去重 + 跨板块去重（核心逻辑）✅

---

##### 📋 问题场景

**为什么需要去重？**

```
彭博社 RSS Feed（20条新闻）
    ↓
被配置了两次：
    ├── 彭博社（亚太板块）→ 抓取同样的20条 → raw_articles
    └── 彭博社（美欧板块）→ 抓取同样的20条 → raw_articles
```

**结果**：
- `raw_articles` 表中有 40 条彭博社新闻
- 其中部分新闻的 `title_original` 完全相同
- 需要去重，保留最新版本

**⚠️ 重要变更（v2.0）**：
- **旧逻辑**：先板块内去重 → 筛选TOP 10 → 跨板块去重 → 导致某些板块不足10条
- **新逻辑**：先板块内去重 → **跨板块全局去重** → 按板块重新分组 → 筛选TOP 10 → 如果不足10条则候补补齐

---

##### 🎯 去重规则（已确认 ✅）

**规则1：先板块内去重，再跨板块全局去重**

```python
# 步骤1：板块内部去重（保留最新版本）
asia_pacific:
    "Stocks rise..." (版本1, 18:00)  ← 删除
    "Stocks rise..." (版本2, 18:12)  ← 保留（最新）

us_europe:
    "Stocks rise..." (版本1, 18:00)  ← 删除
    "Stocks rise..." (版本2, 18:12)  ← 保留（最新）

# 步骤2：跨板块全局去重（基于 title_original + url）
asia_pacific: "Stocks rise..." (版本2, 18:12)  ← 保留（第一次出现）
us_europe:   "Stocks rise..." (版本2, 18:12)  ← 删除（重复）

# 步骤3：按板块重新分组
asia_pacific: [10条]  ← 如果不足10条,从候补新闻中补齐
us_europe:   [10条]  ← 如果不足10条,从候补新闻中补齐
domestic:    [10条]
```

**关键点**：
- ✅ 先板块内去重（保留最新版本）
- ✅ 再跨板块去重（同一条新闻只在一个板块显示）
- ✅ 保留第一次出现的板块
- ✅ 删除后续板块中的重复
- ✅ 如果某板块不足10条，从该板块的候补新闻中补齐

---

**规则2：去重键的选择**

```python
# 英文新闻
if language == 'en':
    # 使用原始标题去重
    dedupe_key = title_original  # "Asia-Pacific stocks trade mixed as..."
else:
    # 使用中文标题去重
    dedupe_key = title  # "亚太股市涨跌互现..."
```

**原因**：
- 英文新闻的 `title` 初始为空，只有 `title_original`
- 中文新闻的 `title_original` 为空，只有 `title`

---

**规则3：保留最新的版本**

```python
# 同一条新闻被多次抓取（因为网络延迟等原因）
article_v1 = {
    'title_original': 'Stocks rise...',
    'crawl_time': '2026-01-26 18:00:00',  # 早期抓取
    'publish_time': '2026-01-26 17:55:00'
}

article_v2 = {
    'title_original': 'Stocks rise...',
    'crawl_time': '2026-01-26 18:12:00',  # 最新抓取
    'publish_time': '2026-01-26 17:55:00'  # 发布时间相同
}

# 保留 article_v2（crawl_time 最新的）
```

**为什么按 `crawl_time` 而不是 `publish_time`？**
- `publish_time`：新闻发布时间（可能相同）
- `crawl_time`：我们的抓取时间（记录抓取顺序）

---

##### 💻 去重逻辑（伪代码）

```python
def deduplicate_by_title(articles):
    """
    在单个板块内去重

    规则：
    1. 英文新闻：根据 title_original 去重
    2. 中文新闻：根据 title 去重
    3. 保留最新的一条（按 crawl_time）
    """
    seen = {}  # {dedupe_key: article}

    for article in articles:
        # 确定去重键
        if article['language'] == 'en':
            key = article['title_original']
        else:
            key = article['title']

        # 如果键为空，跳过
        if not key:
            continue

        # 保留最新的一条（按 crawl_time）
        if key not in seen:
            # 第一次遇到，直接保存
            seen[key] = article
        else:
            # 已经存在，比较时间
            existing_time = seen[key]['crawl_time']
            current_time = article['crawl_time']

            if current_time > existing_time:
                # 当前文章更新，替换
                seen[key] = article

    return list(seen.values())


def cross_category_deduplicate(grouped_articles):
    """
    跨板块全局去重（核心改进 ⭐）

    规则：
    1. 基于 title_original + url 判断是否为同一条新闻
    2. 保留第一次出现的板块
    3. 删除后续板块中的重复

    返回：
    - 去重后的分组新闻
    - 每个板块的候补新闻（用于补齐10条）
    """
    seen = {}  # {(title_original, url): (article, category)}
    deduped = {
        'asia_pacific': [],
        'us_europe': [],
        'domestic': []
    }
    backup = {
        'asia_pacific': [],  # 候补新闻（已去重但未入选）
        'us_europe': [],
        'domestic': []
    }

    # 按板块顺序处理（优先级：domestic > asia_pacific > us_europe）
    category_order = ['domestic', 'asia_pacific', 'us_europe']

    for category in category_order:
        articles = grouped_articles.get(category, [])

        for article in articles:
            # 构建唯一键
            if article['language'] == 'en':
                key = (article['title_original'], article.get('url', ''))
            else:
                key = (article['title'], article.get('url', ''))

            # 检查是否已存在
            if key not in seen:
                # 第一次出现，保留
                seen[key] = (article, category)
                deduped[category].append(article)
            else:
                # 重复新闻，记录为候补
                existing_article, existing_category = seen[key]
                backup[existing_category].append(article)
                logger.info(f"跨板块去重: {article['title']} (已存在于 {existing_category})")

    return deduped, backup


def fill_top_10(deduped_articles, backup_articles):
    """
    填充每个板块到TOP 10（如果不足10条）

    规则：
    1. 每个板块必须有10条新闻
    2. 如果不足10条，从候补新闻中补齐
    3. 候补新闻按关键词评分排序
    """
    final = {}

    for category in ['asia_pacific', 'us_europe', 'domestic']:
        articles = deduped_articles[category]
        backup = backup_articles[category]

        # 如果已有10条，直接使用
        if len(articles) >= 10:
            final[category] = articles[:10]
        else:
            # 不足10条，需要补齐
            needed = 10 - len(articles)
            logger.info(f"{category} 只有 {len(articles)} 条，从候补中补齐 {needed} 条")

            # 从候补新闻中选择评分最高的
            backup_sorted = sorted(
                backup,
                key=lambda x: calculate_priority_score(x),
                reverse=True
            )
            articles.extend(backup_sorted[:needed])
            final[category] = articles

    return final
```

---

##### 📊 实际数据示例

**输入数据（raw_articles - asia_pacific 板块）**

```
总共 68 条新闻：

1. 彭博社: "Stocks rise to record high" (crawl_time: 18:00)
2. 彭博社: "Fed signals rate cuts" (crawl_time: 18:01)
3. 彭博社: "Oil prices surge" (crawl_time: 18:02)
...
19. 彭博社: "Stocks rise to record high" (crawl_time: 18:12)  ← 与第1条重复
20. CNBC: "Market rally continues" (crawl_time: 18:13)
...
38. 彭博社: "Stocks rise to record high" (crawl_time: 18:25)  ← 与第1、19条重复
...
69. Yahoo: "Tech earnings beat estimates" (crawl_time: 18:30)
```

**去重后（asia_pacific 板块）**

```
总共 ~64 条新闻（去掉了 4 条重复的）：

1. 彭博社: "Stocks rise to record high" (crawl_time: 18:25)  ← 保留最新的
2. 彭博社: "Fed signals rate cuts" (crawl_time: 18:01)
3. 彭博社: "Oil prices surge" (crawl_time: 18:02)
...
20. CNBC: "Market rally continues" (crawl_time: 18:13)
...
68. Yahoo: "Tech earnings beat estimates" (crawl_time: 18:30)
```

---

##### 🤔 可能的问题和解决方案

**问题1：如果新闻真的更新了怎么办？**

**场景**：
```
18:00 抓取: "Stocks rise 1%"  ← 旧版本
18:12 抓取: "Stocks rise 2%"  ← 新版本（内容更新了）
```

**当前逻辑**：
- 会认为是同一条新闻（title_original 相同或相似）
- 保留最新的（18:12 的版本）

**评估**：✅ 符合预期（保留最新内容）

---

**问题2：不同源报道同一事件**

**场景**：
```python
# 彭博社
title_original: "Fed signals rate cuts in 2026"

# CNBC
title_original: "Fed Signals Rate Cuts Are Coming in 2026"
```

**当前逻辑**：
- 不会去重（标题不同）
- 两条都会保留

**评估**：✅ 合理（不同媒体的报道角度不同）

---

**问题3：中文新闻重复**

**场景**：
```python
# 新华社
title: "央行降息0.25个百分点"

# 财新网
title: "央行宣布降息25个基点"
```

**当前逻辑**：
- 不会去重（标题完全不同）
- 两条都会保留

**评估**：✅ 合理（表述不同但信息相同，用户可以自行判断）

---

##### 📈 去重效果预测

**板块内部去重**：
```
亚太板块: 68 条 → ~64 条
美欧板块: 68 条 → ~64 条
国内板块: 20 条 → 20 条
```

**跨板块去重**：
```
假设亚太和美欧有 3 条重复新闻：
- 删除亚太板块的 0 条（保留优先级）
- 删除美欧板块的 3 条

结果：
  asia_pacific: ~64 条
  us_europe: ~61 条 (64 - 3)
  domestic: 20 条
```

**整体效果**：
```
去重前总计: 156 条
板块内部去重: ~148 条
跨板块去重: ~145 条
删除率: ~7%（删除了约11条重复）
```

---

##### ✅ 总结

**去重的核心逻辑（v2.0改进版）**：

1. ✅ **两阶段去重**
   - 阶段1：板块内部去重（保留最新版本）
   - 阶段2：跨板块全局去重（保留第一次出现）

2. ✅ **避免数量萎缩**
   - 去重后如果某板块不足10条，从候补新闻中补齐
   - 确保每个板块都有10条新闻

3. ✅ **跨板块去重规则**
   - 基于 `title_original + url` 判断重复
   - 保留优先级：domestic > asia_pacific > us_europe
   - 删除后续板块中的重复

4. ✅ **候补补齐机制**
   - 被跨板块去重删除的新闻成为候补
   - 按关键词评分排序
   - 优先补充高分候补新闻

**状态**：✅ 逻辑已确认，可以实施

---

#### 2.4 筛选重要新闻（每个板块 TOP 10）✅

---

##### 📋 筛选策略：关键词优先级（已确认 ✅）

**核心思路**：
```
1. 先按关键词分类（高/中/低优先级）
2. 每个类别内按时间排序
3. 优先选择高优先级新闻
4. 最终每个板块筛选 TOP 10
```

---

##### 🎯 关键词体系总览（已确认 ✅）

**评分公式**：
```
新闻得分 = 基础分(10) + 通用关键词得分 + 板块特有关键词得分
```

**匹配规则**：
- 标题匹配：+20分（高权重）
- 内容匹配：+10分（低权重）
- 板块特有关键词：
  - 国内板块：+10分
  - 亚太板块：+10分
  - 美欧板块：+15分（权重更高）

---

##### 📋 1️⃣ 通用关键词（所有板块共享）

**【高优先级】通用关键词（48个）**

**中文关键词（23个）**：
- 央行、欧洲央行、日本央行、英国央行
- 利率、降息、加息、基准利率、负利率
- GDP、国内生产总值、经济增速、经济衰退
- 通胀、通货膨胀、CPI、消费者物价指数、通缩
- 财政政策、货币政策、量化宽松、缩表

**英文关键词（25个）**：
- Central Bank, ECB, BOJ, Bank of England
- Interest Rate, Rate Cut, Rate Hike, Benchmark Rate, Negative Rate
- GDP, Economic Growth, Recession
- Inflation, CPI, Deflation
- Monetary Policy, Fiscal Policy, QE, Balance Sheet Reduction

---

**【中优先级】通用关键词（56个）**

**中文关键词（28个）**：
- 股市、大盘、指数、涨跌、震荡
- 上证指数、深证成指、标普500、纳斯达克
- 道琼斯、日经指数、恒生指数
- 牛市、熊市、反弹、回调
- 财报、营收、利润、季度、年报
- 并购、收购、IPO、上市、退市
- CEO、高管、董事会、股东大会
- 油价、黄金、白银、铜、铝
- 原油、期货、大宗商品

**英文关键词（28个）**：
- Stock Market, Index, Rally, Drop, Volatility
- S&P 500, Nasdaq, Dow Jones
- Nikkei, Hang Seng, FTSE
- Bull Market, Bear Market, Rebound, Correction
- Earnings, Revenue, Profit, Quarterly, Annual Report
- M&A, Acquisition, IPO, Listing, Delisting
- CEO, Executive, Board of Directors, Shareholder Meeting
- Oil Price, Gold, Silver, Copper, Aluminum
- Crude Oil, Futures, Commodities

**通用关键词小计：104个**

---

##### 🌏 2️⃣ 板块特有关键词

### 🇨🇳 【国内金融板块】（domestic）特有关键词

**加分规则**：匹配到 +10分

**中文关键词（20个）**：

**国家/地区（5个）**：
- 中国、内地、国内、全国、大陆

**政府机构（7个）**：
- 国务院、央行、证监会、银保监会
- 财政部、发改委、统计局

**政策相关（8个）**：
- 政府政策、政策发布、政策措施
- 国企、民营企业、中小企业
- 改革开放、经济特区、去杠杆

**英文关键词（15个）**：

**国家/地区（4个）**：
- China, Chinese, Mainland, Domestic

**政府机构（6个）**：
- State Council, PBOC, CSRC, CBIRC
- Ministry of Finance, NDRC

**政策相关（5个）**：
- Government Policy, SOE, Private Sector
- Reform and Opening, Economic Zone

**国内板块特有关键词小计：35个**

---

### 🇯🇵 【亚太日本板块】（asia_pacific）特有关键词

**加分规则**：匹配到 +10分

**中文关键词（30个）**：

**国家/地区（15个）**：
- 日本、韩国、印度、澳大利亚、新西兰
- 新加坡、泰国、马来西亚、印尼、菲律宾
- 越南、香港、台湾、亚太、亚洲
- 东盟

**日本特有（8个）**：
- 日本央行、日银、安倍经济学
- 日经指数、东证指数、日元
- 东京、大阪、日本经济

**亚太特有（7个）**：
- 亚太股市、亚太地区、APEC
- RCEP、亚太经合组织、区域合作、东盟峰会

**英文关键词（35个）**：

**国家/地区（15个）**：
- Japan, Korea, India, Australia, New Zealand
- Singapore, Thailand, Malaysia, Indonesia, Philippines
- Vietnam, Hong Kong, Taiwan, Asia-Pacific, Asia
- ASEAN

**日本特有（10个）**：
- BOJ, Bank of Japan, Abenomics
- Nikkei, TOPIX, Yen, JPY
- Tokyo, Osaka, Japanese Economy

**亚太特有（10个）**：
- Asia-Pacific Market, ASEAN Summit
- APEC, RCEP, Regional Cooperation
- Asian Development Bank, APAC

**亚太板块特有关键词小计：65个**

---

### 🇺🇸 【美国欧洲板块】（us_europe）特有关键词

**加分规则**：匹配到 +15分（权重更高）

**中文关键词（25个）**：

**美国（15个）**：
- 美国、美利坚、美股、华尔街
- 美联储、特朗普、拜登、白宫
- 美元、美债、美股、纳斯达克
- 标普500、道琼斯、美国经济
- 贸易战、关税、制裁

**欧洲（10个）**：
- 欧洲、欧盟、欧元、欧洲央行
- 德国、法国、英国、伦敦
- 欧债、Brexit、脱欧、欧洲经济

**英文关键词（40个）**：

**美国（25个）**：
- US, USA, United States, America, Wall Street
- Federal Reserve, Fed, Trump, Biden, White House
- Dollar, US Treasury, US Stock, Nasdaq
- S&P 500, Dow Jones, US Economy
- Trade War, Tariff, Sanction

**欧洲（15个）**：
- Europe, EU, Euro, ECB
- Germany, France, UK, London
- European Debt, Brexit, European Economy

**美欧板块特有关键词小计：65个**

---

##### 📊 关键词统计汇总

| 类型 | 中文 | 英文 | 小计 |
|------|------|------|------|
| **通用高优先级** | 23 | 25 | 48 |
| **通用中优先级** | 28 | 28 | 56 |
| **国内板块特有** | 20 | 15 | 35 |
| **亚太板块特有** | 30 | 35 | 65 |
| **美欧板块特有** | 25 | 40 | 65 |
| **总计** | **126** | **143** | **269** |

**关键词体系结构**：
```
总关键词数：269个
├─ 通用关键词：104个
│  ├─ 高优先级：48个
│  └─ 中优先级：56个
└─ 板块特有关键词：165个
   ├─ 国内板块：35个（加分+10）
   ├─ 亚太板块：65个（加分+10）
   └─ 美欧板块：65个（加分+15）
```

---

##### 🌐 匹配规则（已确认 ✅）

**匹配范围**：
```python
# 同时在标题和内容中匹配
title = (article.get('title_original') or article.get('title') or '').lower()
content = (article.get('content_original') or article.get('content') or '').lower()

# 标题匹配权重更高
for keyword in keywords:
    if keyword in title:
        score += 20  # 标题匹配：高分
    elif keyword in content:
        score += 10  # 内容匹配：低分
```

**匹配优先级**：
1. ✅ 标题匹配：优先，得分高（20分）
2. ✅ 内容匹配：补充，得分低（10分）
3. ✅ 中英文关键词：同时支持
4. ✅ 大小写不敏感：全部转为小写匹配
5. ✅ 板块特有关键词：额外加分（国内+10，亚太+10，美欧+15）

**评分示例**：
```python
# 示例1：国内板块新闻
标题："中国央行宣布降息0.25个百分点，提振股市"
匹配：
  基础分：10
  通用：央行(+20) + 降息(+20) + 股市(+20) = +60
  板块特有：中国(+10)
  总分：10 + 60 + 10 = 80分 ✅ 高优先级

# 示例2：亚太板块新闻
标题："Japan's BOJ Maintains Interest Rate, Asian Markets Rally"
匹配：
  基础分：10
  通用：Interest Rate(+20) + Markets(+20) = +40
  板块特有：Japan(+10) + BOJ(+10) = +20
  总分：10 + 40 + 20 = 70分 ✅ 高优先级

# 示例3：美欧板块新闻
标题："美联储主席鲍威尔暗示可能降息，美股大涨"
匹配：
  基础分：10
  通用：降息(+20) + 美股(+20) = +40
  板块特有：美联储(+15) + 美股(+15) = +30
  总分：10 + 40 + 30 = 80分 ✅ 高优先级
```

---

##### 💻 筛选逻辑（伪代码）

```python
def select_top_news(articles, top_n=10):
    """
    根据关键词优先级筛选新闻

    策略：
    1. 计算每条新闻的优先级得分
    2. 按优先级分组（高/中/低）
    3. 每组内按时间排序
    4. 按比例选择（高:中:低 = 5:3:2）
    5. 最终总数为 top_n
    """

    # 第1步：计算每条新闻的优先级得分
    scored_articles = []
    for article in articles:
        score = calculate_priority_score(article)
        scored_articles.append({
            'article': article,
            'score': score,
            'publish_time': article['publish_time']
        })

    # 第2步：按优先级分组
    high_priority = []   # 得分 >= 80
    medium_priority = [] # 得分 40-79
    low_priority = []    # 得分 < 40

    for item in scored_articles:
        if item['score'] >= 80:
            high_priority.append(item)
        elif item['score'] >= 40:
            medium_priority.append(item)
        else:
            low_priority.append(item)

    # 第3步：每组内按时间排序（最新的在前）
    for group in [high_priority, medium_priority, low_priority]:
        group.sort(key=lambda x: x['publish_time'], reverse=True)

    # 第4步：按比例选择（高:中:低 = 5:3:2）
    # TOP 10 = 5条高优先级 + 3条中优先级 + 2条低优先级
    high_count = int(top_n * 0.5)    # 5条
    medium_count = int(top_n * 0.3)  # 3条
    low_count = top_n - high_count - medium_count  # 2条

    selected = []
    selected.extend([item['article'] for item in high_priority[:high_count]])
    selected.extend([item['article'] for item in medium_priority[:medium_count]])
    selected.extend([item['article'] for item in low_priority[:low_count]])

    # 如果某组数量不足，从其他组补充
    if len(selected) < top_n:
        # 按优先级和时间顺序补充
        remaining = scored_articles
        remaining.sort(key=lambda x: (x['score'], x['publish_time']), reverse=True)

        for item in remaining:
            if item['article'] not in selected:
                selected.append(item['article'])
                if len(selected) >= top_n:
                    break

    return selected


def calculate_priority_score(article):
    """
    计算新闻的优先级得分（v2.0改进版 ⭐）

    改进点：
    1. 设置关键词得分上限，防止SEO堆砌
    2. 标题匹配得分上限：+40分（最多2个关键词）
    3. 内容匹配得分上限：+20分（最多2个关键词）
    4. 总分上限：60分（避免垃圾新闻高分）
    """
    title = (article.get('title_original') or article.get('title') or '').lower()
    content = (article.get('content_original') or article.get('content') or '').lower()

    score = 10  # 基础分

    # 高优先级关键词（标题+20，内容+10）
    high_keywords = [
        '央行', '美联储', '欧洲央行', '日本央行',
        '利率', '降息', '加息', '基准利率',
        'gdp', '国内生产总值', '经济增速',
        '通胀', '通货膨胀', 'cpi', '消费者物价',
        '财政政策', '货币政策', '量化宽松', 'qe'
    ]

    # ⭐ 标题匹配：最多2个关键词，上限+40分
    title_high_count = 0
    for keyword in high_keywords:
        if keyword in title:
            score += 20
            title_high_count += 1
            if title_high_count >= 2:  # 最多2个关键词
                break

    # ⭐ 内容匹配：最多1个关键词，上限+10分
    content_high_count = 0
    for keyword in high_keywords:
        if keyword in content:
            score += 10
            content_high_count += 1
            if content_high_count >= 1:  # 最多1个关键词
                break

    # 中优先级关键词（标题+10，内容+5）
    medium_keywords = [
        '股市', '大盘', '指数', '涨跌',
        '财报', '营收', '利润', '季度',
        '并购', '收购', 'ipo', '上市',
        '油价', '黄金', '白银', '铜', '原油'
    ]

    # ⭐ 标题匹配：最多2个关键词，上限+20分
    title_medium_count = 0
    for keyword in medium_keywords:
        if keyword in title:
            score += 10
            title_medium_count += 1
            if title_medium_count >= 2:  # 最多2个关键词
                break

    # ⭐ 内容匹配：最多1个关键词，上限+5分
    content_medium_count = 0
    for keyword in medium_keywords:
        if keyword in content:
            score += 5
            content_medium_count += 1
            if content_medium_count >= 1:  # 最多1个关键词
                break

    # ⭐ 总分上限：60分（避免SEO堆砌）
    return min(score, 60)
```

---

##### 📊 筛选示例

**输入数据**：
```
亚太板块去重后：64条新闻

分类结果：
【高优先级】：15条
  - 央行相关：5条
  - 利率相关：4条
  - GDP/通胀：6条

【中优先级】：30条
  - 股市动态：15条
  - 财报/IPO：10条
  - 大宗商品：5条

【低优先级】：19条
  - 其他财经新闻
```

**筛选过程**：
```
步骤1：按优先级和时间排序
  高优先级（按时间排序）：15条
  中优先级（按时间排序）：30条
  低优先级（按时间排序）：19条

步骤2：按比例选择（5:3:2）
  高优先级：选最新的 5条 ✅
  中优先级：选最新的 3条 ✅
  低优先级：选最新的 2条 ✅

步骤3：总计
  5 + 3 + 2 = 10条
```

**输出结果**：
```python
{
    'asia_pacific': [10条精选新闻],
    'us_europe': [10条精选新闻],
    'domestic': [10条精选新闻]
}
# 总计 30 条精选新闻
```

---

##### 🤔 特殊情况处理

**情况1：高优先级新闻不足5条**

```
高优先级只有：3条
中优先级有：30条
低优先级有：19条

处理：
  高优先级：选3条
  中优先级：选5条（补充2条）
  低优先级：选2条
  总计：3 + 5 + 2 = 10条
```

**情况2：全部是低优先级**

```
某个板块没有重要新闻，全部是低优先级

处理：
  直接按时间排序，选最新的10条
  （保证每个板块都有足够的新闻）
```

**情况3：新闻总数不足10条**

```
某个板块去重后只有8条新闻

处理：
  全部保留，只筛选8条
  （不强求凑够10条）
```

---

##### 📈 筛选效果预测

**亚太板块**：
```
去重后：64条
├─ 高优先级：15条 → 筛选后：5条
├─ 中优先级：30条 → 筛选后：3条
└─ 低优先级：19条 → 筛选后：2条

总计：10条
```

**美欧板块**：
```
去重后：64条
└─ 同样的逻辑

总计：10条
```

**国内板块**：
```
去重后：20条
├─ 高优先级：8条 → 筛选后：5条
├─ 中优先级：8条 → 筛选后：3条
└─ 低优先级：4条 → 筛选后：2条

总计：10条
```

**整体效果**：
```
去重后总计：~148条
筛选后总计：30条（每个板块10条）
筛选率：~20%
```

---

##### ✅ 总结

**筛选标准**：
1. ✅ 基于关键词优先级（高/中/低）
2. ✅ 高优先级优先选择
3. ✅ 每组内按时间排序
4. ✅ 按比例选择（5:3:2）
5. ✅ 每个板块最终筛选 TOP 10

**优势**：
- ✅ 保证重要新闻不被遗漏
- ✅ 平衡时效性和重要性
- ✅ 每个板块数量统一（10条）
- ✅ 总新闻量充足（30条 vs 原来的15条）

**状态**：✅ 方案已确认，可以实施

---

#### 2.5 翻译英文新闻（防重复机制）✅

---

##### 🎯 翻译需求

**目标**：
- 将英文新闻翻译成中文
- 每篇文章只翻译一次
- 不重复调用API（控制成本）
- 翻译结果持久化保存

**挑战**：
- 同一条新闻可能在多次运行中重复出现
- API调用成本高（每次$0.002-0.005）
- 需要记录翻译状态

---

##### 📊 翻译状态机

**状态定义**：

```
未翻译（Pending）→ 翻译中（Translating）→ 已翻译（Completed）
                                ↓
                           翻译失败（Failed）
```

**状态说明**：

1. **未翻译（Pending）**
   - 初始状态
   - `translated = False`
   - `title = None` 或空字符串
   - `translation_method = None`

2. **翻译中（Translating）**
   - 临时状态（程序内部）
   - 正在调用Claude API
   - 防止并发翻译同一条新闻

3. **已翻译（Completed）**
   - 翻译成功
   - `translated = True`
   - `title = "翻译后的中文标题"`
   - `translation_method = "claude"`
   - `translated_at = "2026-01-26 18:15:30"`

4. **翻译失败（Failed）**
   - API调用失败
   - `translated = False`
   - `translation_error = "错误信息"`
   - 可以重试

---

##### 🔒 防重复翻译机制

**核心原则**：
```
每条新闻（根据唯一标识）在其生命周期内，只调用一次翻译API
```

**唯一标识**：
```python
# 英文新闻的唯一标识
unique_id = f"{article['url']}_{article['title_original'][:100]}"

# 或者使用数据库ID（如果已保存）
unique_id = article['id']  # 数据库主键
```

---

##### 💡 翻译触发条件

**什么时候需要翻译？**

```
条件1：语言是英文
  AND
条件2：标题为空 或 translated=False
  AND
条件3：之前未成功翻译过
```

**判断逻辑**：

```python
# 伪代码
should_translate(article):
    # 条件1：检查语言
    if article['language'] != 'en':
        return False  # 中文新闻不需要翻译

    # 条件2：检查是否已翻译
    if article['translated'] == True and article['title']:
        return False  # 已有翻译结果

    # 条件3：检查翻译状态
    if article.get('translation_error'):
        # 之前失败过，是否重试？
        if can_retry(article):
            return True  # 允许重试
        else:
            return False  # 超过重试次数，放弃

    # 条件4：检查是否正在翻译中
    if article.get('translating') == True:
        return False  # 正在被其他进程翻译

    return True  # 需要翻译
```

---

##### 🗄️ 数据库设计（翻译相关字段）

**raw_articles 临时表**：
```sql
CREATE TABLE raw_articles (
    -- 基础字段
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    title TEXT,                      -- 翻译后的标题（初始为NULL）
    title_original TEXT,             -- 原始标题（英文）

    -- 翻译状态字段
    translated BOOLEAN DEFAULT 0,    -- 是否已翻译
    translating BOOLEAN DEFAULT 0,   -- 是否正在翻译中（锁）
    translation_method TEXT,         -- 翻译方法：claude
    translated_at TIMESTAMP,         -- 翻译时间
    translation_error TEXT,          -- 翻译错误信息
    retry_count INTEGER DEFAULT 0,   -- 重试次数

    -- 其他字段...
    source TEXT,
    category TEXT,
    language TEXT,
    crawl_time TIMESTAMP
)
```

**news_articles 正式表**：
```sql
CREATE TABLE news_articles (
    -- 基础字段
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    title TEXT NOT NULL,              -- 翻译后的标题
    title_original TEXT,             -- 原始标题（英文）

    -- 翻译状态字段
    translated BOOLEAN DEFAULT 0,    -- 是否已翻译
    translation_method TEXT,         -- 翻译方法：claude
    translated_at TIMESTAMP,         -- 翻译完成时间

    -- 唯一约束
    CONSTRAINT unique_title UNIQUE (title, title_original)
)
```

---

##### 🔄 翻译流程详解

**场景1：首次翻译（新文章）**

```
步骤1：从临时表读取新闻
  article = {
      'id': 123,
      'title': None,           # 未翻译
      'title_original': 'Fed Signals Rate Cuts',
      'translated': False,
      'language': 'en'
  }

步骤2：检查是否需要翻译
  ✅ language == 'en'
  ✅ translated == False
  ✅ title == None
  → 需要翻译

步骤3：设置翻译中锁（防止并发）
  UPDATE raw_articles
  SET translating = 1
  WHERE id = 123

步骤4：调用Claude API
  response = claude_api.translate(article['title_original'])
  → "美联储暗示降息"

步骤5：保存翻译结果
  UPDATE raw_articles
  SET
    title = '美联储暗示降息',
    translated = 1,
    translating = 0,
    translation_method = 'claude',
    translated_at = CURRENT_TIMESTAMP
  WHERE id = 123

步骤6：记录日志
  logger.info("✓ 已翻译: Fed Signals Rate Cuts → 美联储暗示降息")
```

---

**场景2：重复运行（已翻译过）**

```
步骤1：从临时表读取新闻
  article = {
      'id': 123,
      'title': '美联储暗示降息',  # 已有翻译
      'title_original': 'Fed Signals Rate Cuts',
      'translated': True,
      'translated_at': '2026-01-26 18:15:30',
      'language': 'en'
  }

步骤2：检查是否需要翻译
  ✅ language == 'en'
  ❌ translated == True  # 已翻译
  → 跳过翻译

步骤3：使用已有的翻译结果
  logger.info("⊙ 使用缓存翻译: 美联储暗示降息")
  无API调用，节省成本
```

---

**场景3：翻译失败重试**

```
步骤1：首次翻译失败
  article = {
      'id': 123,
      'title': None,
      'title_original': 'Fed Signals Rate Cuts',
      'translated': False,
      'translation_error': 'API Error: Rate limit exceeded',
      'retry_count': 0
  }

步骤2：检查是否重试
  if article['retry_count'] < MAX_RETRY:
      retry_count += 1
      → 重新翻译
  else:
      → 放弃翻译，记录日志

步骤3：重试翻译
  调用API → 成功
  保存结果
  清除错误信息
```

---

##### 🚫 防并发翻译机制

**问题**：
```
如果同时运行多个进程，可能会重复翻译同一条新闻

进程A: 检查article[123] → 未翻译 → 开始翻译
进程B: 检查article[123] → 未翻译 → 也开始翻译 ❌
结果：重复调用API，浪费成本
```

**解决方案：数据库锁 + BEGIN IMMEDIATE事务（v2.0改进 ⭐）**

```python
# 伪代码（使用事务保证原子性）
def translate_with_lock(article_id):
    """
    使用事务和锁机制防止并发翻译

    v2.0改进：
    1. 使用BEGIN IMMEDIATE事务
    2. 原子性地获取锁
    3. 异常时自动释放锁
    """
    try:
        # 步骤1：开始IMMEDIATE事务（独占锁）
        db.execute("BEGIN IMMEDIATE")

        # 步骤2：原子性地获取翻译锁
        success = db.set_translating_lock(article_id)

        if not success:
            # 锁已被其他进程获取
            db.execute("ROLLBACK")
            logger.info("新闻正在被翻译，跳过")
            return None

        try:
            # 步骤3：执行翻译
            result = claude_api.translate(article)

            # 步骤4：保存结果
            db.save_translation(article_id, result)

            # 步骤5：释放锁并提交事务
            db.release_translating_lock(article_id)
            db.execute("COMMIT")

            return result

        except Exception as e:
            # 翻译失败，回滚事务并释放锁
            db.execute("ROLLBACK")
            logger.error(f"翻译失败: {e}")
            raise

    except Exception as e:
        # 事务级别错误
        logger.error(f"事务错误: {e}")
        raise
```

**锁实现（数据库层面）**：
```sql
-- 设置翻译锁（原子操作）
UPDATE raw_articles
SET translating = 1,
    translation_locked_at = CURRENT_TIMESTAMP
WHERE id = 123
  AND translating = 0  -- 只有未锁定时才成功

-- 检查是否成功（在事务内执行）
SELECT changes()  -- 如果返回1，表示获取锁成功；返回0，表示已被锁定

-- 释放锁（在COMMIT前执行）
UPDATE raw_articles
SET translating = 0
WHERE id = 123
```

**v2.0改进说明**：
- ✅ **BEGIN IMMEDIATE**：立即获取数据库写锁，防止其他进程同时写入
- ✅ **原子性操作**：检查锁 + 设置锁 在一个事务内完成
- ✅ **异常安全**：无论成功失败，都会正确释放锁和事务
- ✅ **并发安全**：SQLite保证IMMEDIATE事务的串行化执行

---

##### 📋 翻译队列管理

**批量翻译逻辑**：

```
待翻译新闻列表（20条）
    ↓
分组（每批5条，并发控制）
    ↓
批量1: [1-5条]   → 并发翻译 → 等待完成
批量2: [6-10条]  → 并发翻译 → 等待完成
批量3: [11-15条] → 并发翻译 → 等待完成
批量4: [16-20条] → 并发翻译 → 等待完成
```

**优势**：
- ✅ 控制并发数，避免API限流
- ✅ 失败只影响单条，不影响整批
- ✅ 可以为每批设置超时时间

---

##### 💰 成本控制

**成本预估**：
```
翻译数量：~20条英文新闻/天
API成本：~$0.002/条
日成本：~$0.04
月成本：~$1.2（~8.5元）
```

**成本优化策略**：

1. **去重优先**
   ```
   先去重，再翻译
   → 20条去重后可能只有15条需要翻译
   ```

2. **缓存复用**
   ```
   已翻译的新闻永久保存
   → 重复运行不再调用API
   ```

3. **批量处理**
   ```
   合并多个新闻，批量调用API
   → 减少网络开销
   ```

4. **失败快速放弃**
   ```
   单条新闻重试3次失败后放弃
   → 避免无限重试浪费成本
   ```

---

##### 📊 翻译统计和监控

**统计指标**：

```python
# 翻译统计
translation_stats = {
    'total_articles': 30,          # 总新闻数
    'need_translation': 20,        # 需要翻译
    'already_translated': 5,       # 已翻译（缓存）
    'newly_translated': 13,        # 本次新翻译
    'failed': 2,                   # 翻译失败
    'skipped': 10                  # 中文新闻跳过
}

# 成本统计
cost_stats = {
    'api_calls': 13,               # API调用次数
    'estimated_cost': 0.026,       # 预估成本（$）
    'saved_cost': 0.01             # 节省的成本（使用缓存）
}
```

**日志记录**：

```python
logger.info("翻译统计:")
logger.info(f"  总新闻数: {stats['total_articles']}")
logger.info(f"  需要翻译: {stats['need_translation']}")
logger.info(f"  使用缓存: {stats['already_translated']} ✅")
logger.info(f"  新翻译: {stats['newly_translated']}")
logger.info(f"  翻译失败: {stats['failed']}")
logger.info(f"  API调用: {stats['api_calls']}次")
logger.info(f"  预估成本: ${stats['estimated_cost']:.3f}")
logger.info(f"  节省成本: ${stats['saved_cost']:.3f}")
```

---

##### ✅ 翻译需求总结

**核心要求**：

1. ✅ **每条新闻只翻译一次**
   - 通过`translated`字段标记
   - 通过`translating`字段防止并发
   - 保存翻译结果永久复用

2. ✅ **不重复调用API**
   - 翻译前检查`translated`状态
   - 使用数据库锁防止并发
   - 失败后限制重试次数

3. ✅ **翻译结果持久化**
   - 保存到`raw_articles`临时表
   - 最终保存到`news_articles`正式表
   - 记录翻译时间和方法

4. ✅ **错误处理和重试**
   - 记录错误信息
   - 最多重试3次
   - 超过次数放弃并记录日志

5. ✅ **成本控制**
   - 使用缓存节省成本
   - 批量处理提高效率
   - 统计和监控API使用

**预期效果**：
```
首次运行：翻译20条 → API调用20次 → 成本$0.04
重复运行：翻译20条 → API调用0次（使用缓存） → 成本$0 ✅
```

**状态**：✅ 逻辑已确认，可以实施

---

#### 2.6 生成AI评论（防重复机制）✅

---

##### 🎯 AI评论需求

**目标**：
- 为每条精选新闻生成专业的金融评论
- 每篇文章只生成一次评论
- 不重复调用API（成本更高）
- 评论结果持久化保存

**挑战**：
- AI评论成本比翻译高3-5倍
- 同一条新闻可能多次出现
- 需要等待翻译完成后才能生成评论
- 评论具有时效性但可以复用

---

##### 📊 AI评论状态机

**状态定义**：

```
未生成（Pending）→ 生成中（Generating）→ 已生成（Completed）
                                ↓
                            生成失败（Failed）
```

**状态说明**：

1. **未生成（Pending）**
   - 初始状态
   - `ai_comment = NULL` 或空字符串
   - `comment_generated = False`
   - `comment_method = None`

2. **生成中（Generating）**
   - 临时状态（程序内部）
   - 正在调用Claude API生成评论
   - 防止并发生成同一条新闻的评论

3. **已生成（Completed）**
   - 评论生成成功
   - `ai_comment = "专业的金融评论内容"`
   - `comment_generated = True`
   - `comment_method = "claude"`
   - `comment_generated_at = "2026-01-26 18:20:30"`

4. **生成失败（Failed）**
   - API调用失败
   - `comment_generated = False`
   - `comment_error = "错误信息"`
   - 可以重试

---

##### 🔒 防重复生成机制

**核心原则**：
```
每条新闻（根据唯一标识）在其生命周期内，只调用一次AI评论API
```

**唯一标识**：
```
使用数据库ID作为唯一标识
unique_id = article['id']
```

---

##### 💡 AI评论触发条件

**什么时候需要生成AI评论？**

```
条件1：新闻已完成翻译（如果需要翻译）
  AND
条件2：评论为空或 comment_generated=False
  AND
条件3：之前未成功生成过
  AND
条件4：未被其他进程锁定
```

**与翻译的区别**：

**翻译**：
- 只需要检查：语言 + 翻译状态
- 独立执行，不依赖其他步骤

**AI评论**：
- 需要检查：翻译状态 + 评论状态
- **依赖关系**：英文新闻必须先翻译，才能生成评论
- 执行顺序：翻译 → AI评论 → 保存

**判断逻辑**：

```
步骤1：检查翻译状态
  if article['language'] == 'en' and not article['translated']:
      等待翻译完成
      或者跳过本条新闻

步骤2：检查评论状态
  if article['comment_generated'] == True and article['ai_comment']:
      跳过，使用已有评论

步骤3：检查是否正在生成
  if article['comment_generating'] == True:
      跳过，正在被其他进程处理

步骤4：检查失败重试
  if article.get('comment_error'):
      if can_retry(article):
          重新生成
      else:
          放弃

所有条件通过 → 生成AI评论
```

---

##### 🗄️ 数据库设计（AI评论相关字段）

**raw_articles 临时表**：
```sql
CREATE TABLE raw_articles (
    -- 基础字段
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    title TEXT,
    content TEXT,

    -- AI评论字段
    ai_comment TEXT,                 -- AI评论内容
    comment_generated BOOLEAN DEFAULT 0,  -- 是否已生成
    comment_generating BOOLEAN DEFAULT 0, -- 是否正在生成中（锁）
    comment_method TEXT,              -- 生成方法：claude
    comment_generated_at TIMESTAMP,   -- 生成时间
    comment_error TEXT,               -- 错误信息
    comment_retry_count INTEGER DEFAULT 0, -- 重试次数

    -- 其他字段...
    translated BOOLEAN,               -- 翻译状态（评论依赖此字段）
    source TEXT,
    category TEXT
)
```

**news_articles 正式表**：
```sql
CREATE TABLE news_articles (
    -- 基础字段
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    title TEXT NOT NULL,
    content TEXT,

    -- AI评论字段
    ai_comment TEXT,                  -- AI评论内容
    comment_generated BOOLEAN DEFAULT 1, -- 默认为True（保存时应有评论）
    comment_method TEXT,              -- 生成方法：claude
    comment_generated_at TIMESTAMP,   -- 生成时间

    -- 唯一约束
    CONSTRAINT unique_title UNIQUE (title, title_original)
)
```

---

##### 🔄 AI评论流程详解

**场景1：首次生成（新文章）**

```
步骤1：检查翻译状态
  article = {
      'id': 123,
      'title': '美联储暗示降息',  # 已翻译
      'translated': True,
      'ai_comment': None,
      'comment_generated': False
  }
  ✅ 翻译已完成，可以生成评论

步骤2：设置生成锁
  UPDATE raw_articles
  SET comment_generating = 1
  WHERE id = 123

步骤3：准备Prompt
  prompt = f"""
  新闻标题：{article['title']}
  新闻来源：{article['source']}
  请从专业金融分析师的角度，为这条新闻写一段30-50字的锐评。
  """

步骤4：调用Claude API
  response = claude_api.generate_comment(prompt)
  → "美联储释放降息信号，市场流动性预期升温，利好股市"

步骤5：保存评论结果
  UPDATE raw_articles
  SET
    ai_comment = '美联储释放降息信号，市场流动性预期升温，利好股市',
    comment_generated = 1,
    comment_generating = 0,
    comment_method = 'claude',
    comment_generated_at = CURRENT_TIMESTAMP
  WHERE id = 123

步骤6：记录日志
  logger.info("✓ 已生成AI评论: 美联储暗示降息")
```

---

**场景2：重复运行（已生成过）**

```
步骤1：检查评论状态
  article = {
      'id': 123,
      'title': '美联储暗示降息',
      'ai_comment': '美联储释放降息信号...',  # 已有评论
      'comment_generated': True,
      'comment_generated_at': '2026-01-26 18:20:30'
  }

步骤2：检查是否需要生成
  ✅ comment_generated == True
  ✅ ai_comment 不为空
  → 跳过生成

步骤3：使用已有的评论
  logger.info("⊙ 使用缓存AI评论: 美联储释放降息信号...")
  无API调用，节省成本
```

---

**场景3：等待翻译完成**

```
步骤1：检查翻译状态
  article = {
      'id': 123,
      'title': None,  # 未翻译
      'title_original': 'Fed Signals Rate Cuts',
      'translated': False,
      'ai_comment': None
  }

步骤2：等待翻译
  if article['language'] == 'en' and not article['translated']:
      logger.info("⏳ 等待翻译完成: Fed Signals Rate Cuts")
      跳过本条新闻，等待翻译完成后再生成评论

步骤3：下次运行时处理
  （翻译已完成，回到场景1）
```

---

**场景4：生成失败重试**

```
步骤1：首次生成失败
  article = {
      'id': 123,
      'comment_error': 'API Error: Rate limit exceeded',
      'comment_retry_count': 0
  }

步骤2：检查是否重试
  if article['comment_retry_count'] < MAX_RETRY:
      retry_count += 1
      → 重新生成
  else:
      → 放弃生成，记录日志

步骤3：重试生成
  调用API → 成功
  保存结果
  清除错误信息
```

---

##### 🚫 防并发生成机制

**问题**：
```
如果同时运行多个进程，可能会重复生成同一条新闻的评论

进程A: 检查article[123] → 未生成 → 开始生成
进程B: 检查article[123] → 未生成 → 也开始生成 ❌
结果：重复调用API，浪费成本（比翻译更贵）
```

**解决方案：数据库锁**

```
设置评论锁：
  UPDATE raw_articles
  SET comment_generating = 1
  WHERE id = 123 AND comment_generating = 0

只有第一条SQL会成功
```

**异常处理**：
```
如果进程崩溃，锁如何释放？

方案1：设置超时时间
  如果 comment_generating = 1
  且 comment_generated_at 超过10分钟
  → 认为进程崩溃，释放锁

方案2：定时清理
  每小时清理一次超时的锁
```

---

##### 📋 AI评论队列管理

**批量生成逻辑**：

```
精选新闻（30条）
    ↓
过滤已生成评论的（10条已有评论）
    ↓
待生成新闻（20条）
    ↓
分组（每批3条，比翻译批次更小）
    ↓
批量1: [1-3条]  → 串行生成 → 等待完成
批量2: [4-6条]  → 串行生成 → 等待完成
批量3: [7-9条]  → 串行生成 → 等待完成
...
```

**为什么批次更小？**
- AI评论比翻译慢（prompt更长，输出更长）
- AI评论成本更高（需要更谨慎）
- 串行处理避免并发冲突

---

##### 💰 成本控制

**成本预估**：
```
评论数量：~30条精选新闻/天
API成本：~$0.005/条（比翻译贵2-3倍）
日成本：~$0.15
月成本：~$4.5（~32元）
```

**成本优化策略**：

1. **缓存复用（最重要）**
   ```
   已生成的评论永久保存
   重复运行不再调用API
   节省成本：~100%
   ```

2. **只对精选新闻生成**
   ```
   不是所有156条都生成评论
   只对筛选后的30条精选新闻生成
   节省成本：~80%
   ```

3. **失败快速放弃**
   ```
   单条新闻重试1次（比重试次数少）
   避免无限重试浪费成本
   ```

4. **长度控制**
   ```
   Prompt限制：30-50字
   避免生成过长内容增加成本
   ```

---

##### 📝 AI评论Prompt设计

**Prompt结构**：

```
角色设定：
你是一位专业的金融分析师，擅长从宏观经济和
市场动态的角度分析新闻事件。

任务：
为以下新闻写一段30-50字的锐评，要求：
1. 一针见血，直击要点
2. 专业但不晦涩
3. 有观点但不主观臆断

新闻信息：
- 标题：{title}
- 来源：{source}
- 板块：{category}

请直接输出评论，不需要解释。
```

**Prompt特点**：
- ✅ 角色明确：专业金融分析师
- ✅ 任务清晰：30-50字锐评
- ✅ 约束具体：不长篇大论
- ✅ 有指导性：有观点但不主观

**示例输出**：
```
输入：美联储暗示降息，美股大涨
输出：美联储释放降息信号，市场流动性预期升温，
      短期提振风险偏好，但需关注通胀反弹风险。
```

---

##### 📊 AI评论统计和监控

**统计指标**：

```
评论生成统计：
  总新闻数：30条
  已有评论：5条（缓存）
  需要生成：25条
  等待翻译：3条（英文未翻译）
  新生成：20条
  生成失败：2条

成本统计：
  API调用：20次
  预估成本：$0.10
  节省成本：$0.05（使用缓存）
```

**日志记录**：

```
logger.info("AI评论统计:")
logger.info(f"  总新闻数: 30")
logger.info(f"  使用缓存: 5条 ✅")
logger.info(f"  等待翻译: 3条 ⏳")
logger.info(f"  新生成: 20条 📝")
logger.info(f"  生成失败: 2条 ❌")
logger.info(f"  API调用: 20次")
logger.info(f"  预估成本: ${0.10}")
logger.info(f"  节省成本: ${0.05}")
```

---

##### ⚖️ 翻译 vs AI评论 对比

| 特性 | 翻译 | AI评论 |
|------|------|--------|
| **成本** | ~$0.002/条 | ~$0.005/条 |
| **依赖关系** | 独立 | 依赖翻译 |
| **输入** | title_original | title + 翻译后的内容 |
| **输出** | title（中文） | ai_comment（评论） |
| **缓存策略** | 永久复用 | 永久复用 |
| **批次大小** | 5条/批 | 3条/批 |
| **重试次数** | 最多3次 | 最多1次 |
| **日成本** | ~$0.04 | ~$0.15 |

---

##### ✅ AI评论需求总结

**核心要求**：

1. ✅ **每条新闻只生成一次评论**
   - 通过`comment_generated`字段标记
   - 通过`comment_generating`字段防止并发
   - 保存评论结果永久复用

2. ✅ **依赖翻译完成**
   - 英文新闻必须先翻译
   - 检查`translated`状态
   - 未翻译则跳过或等待

3. ✅ **不重复调用API**
   - 生成前检查状态
   - 使用数据库锁防止并发
   - 失败后限制重试次数

4. ✅ **成本控制**
   - 只对精选新闻生成（30条 vs 156条）
   - 使用缓存节省成本
   - 批次更小（3条 vs 5条）

5. ✅ **质量控制**
   - 设计专业Prompt
   - 控制评论长度（30-50字）
   - 记录失败案例便于分析

**预期效果**：
```
首次运行：
  翻译20条 → 生成30条评论
  API调用：50次（20+30）
  成本：~$0.19

重复运行（同一天）：
  翻译0条（缓存） → 生成0条（缓存）
  API调用：0次
  成本：$0 ✅

节省：100%成本
```

**状态**：✅ 逻辑已确认，可以实施

---

#### 2.7 保存到正式表

```python
def save_to_news_articles(top_news_by_category):
    """保存到正式表"""
    for category, articles in top_news_by_category.items():
        for article in articles:
            # 转换为 NewsArticle 对象
            news_article = NewsArticle(
                title=article.get('title'),
                title_original=article.get('title_original'),
                content=article.get('content'),
                content_original=article.get('content_original'),
                source=article['source'],
                source_original=article.get('source_original'),
                url=article.get('url'),
                category=Category(article['category']),
                language=Language(article['language']),
                publish_time=datetime.fromisoformat(article['publish_time']),
                crawl_time=datetime.fromisoformat(article['crawl_time']),
                ai_comment=article.get('ai_comment'),
                translated=True,
                translation_method='claude',
                featured=True
            )

            # 保存到正式表
            db_manager.save_article(news_article)
```

**注意**：
- `news_articles` 表有唯一约束
- 如果新闻已存在会跳过（避免重复保存）

---

#### 2.8 生成HTML（时效性过滤）✅

---

##### 🎯 HTML生成需求（已确认 ✅）

**核心原则**：
```
每天的HTML只显示24小时内的新新闻
昨天的新闻在昨天的HTML文件中
今天的新闻在今天的HTML文件中
两者互不重叠
```

**运行模式**：
```
程序每天运行一次（定时任务）
每天生成一个独立的HTML文件

1月25日运行 → 生成 2026-01-25.html
1月26日运行 → 生成 2026-01-26.html
1月27日运行 → 生成 2026-01-27.html
...
```

---

##### ⏰ 时效性过滤逻辑

**时间窗口**：

```
每次生成HTML时，定义一个24小时的时间窗口

假设程序在每天18:00运行：

1月25日18:00运行：
  时间窗口：1月24日18:00 → 1月25日18:00
  显示范围：24小时内

1月26日18:00运行：
  时间窗口：1月25日18:00 → 1月26日18:00
  显示范围：24小时内

1月27日18:00运行：
  时间窗口：1月26日18:00 → 1月27日18:00
  显示范围：24小时内
```

**窗口不重叠**：
```
2026-01-25.html: [1月24日18:00, 1月25日18:00]
2026-01-26.html: [1月25日18:00, 1月26日18:00]
                    ↑ 唯一重叠点（边界）
2026-01-27.html: [1月26日18:00, 1月27日18:00]
```

---

##### 🔒 跨天去重机制

**需求**：
> "如果一条新闻已经在昨天出现了，那么在今天就不要再显示"

**实现方式**：时效性过滤

**核心逻辑**：
```
步骤1：获取当前时间
  current_time = now()  # 假设是1月26日18:00

步骤2：计算时间窗口
  start_time = current_time - 24小时  # 1月25日18:00
  end_time = current_time               # 1月26日18:00

步骤3：筛选新闻
  只选择 publish_time 在 [start_time, end_time] 范围内的新闻

步骤4：按板块分组和筛选
  在时间窗口内的新闻
  → 按板块分组
  → 板块内部去重
  → 关键词筛选TOP 10
  → 翻译
  → 生成AI评论

步骤5：生成HTML
  使用筛选后的30条新闻
  生成 2026-01-26.html
```

---

##### 📊 实际场景示例

**场景1：连续三天运行**

```
数据库中的新闻：
  新闻A：发布时间 1月24日10:00
  新闻B：发布时间 1月25日10:00
  新闻C：发布时间 1月26日10:00
  新闻D：发布时间 1月26日17:00
```

**第1天运行（1月25日18:00）**：
```
时间窗口：[1月24日18:00, 1月25日18:00]

筛选结果：
  ✅ 新闻A：1月24日10:00 → 在窗口内 → 显示
  ❌ 新闻B：1月25日10:00 → 在窗口内 → 显示
  ❌ 新闻C：1月26日10:00 → 超出窗口 → 不显示
  ❌ 新闻D：1月26日17:00 → 超出窗口 → 不显示

生成：2026-01-25.html（包含新闻A、新闻B）
```

**第2天运行（1月26日18:00）**：
```
时间窗口：[1月25日18:00, 1月26日18:00]

筛选结果：
  ❌ 新闻A：1月24日10:00 → 超出窗口 → 不显示
  ❌ 新闻B：1月25日10:00 → 超出窗口 → 不显示（已在昨天显示）
  ✅ 新闻C：1月26日10:00 → 在窗口内 → 显示
  ✅ 新闻D：1月26日17:00 → 在窗口内 → 显示

生成：2026-01-26.html（包含新闻C、新闻D）
```

**结果**：
- ✅ 新闻A和B在1月25日的HTML中
- ✅ 新闻C和D在1月26日的HTML中
- ✅ 没有重复显示
- ✅ 每天都是新鲜的24小时内新闻

---

##### 🔄 跨板块去重逻辑

**需求确认**：
> "同一条新闻在不同板块如何处理？跨板块也去重"

**实现方式**：

**场景：同一条新闻出现在多个板块**

```
数据库中的新闻：
  新闻X：发布时间 1月25日10:00
          category: asia_pacific（亚太板块）
          title_original: "BOJ Keeps Rate Unchanged"

  新闻Y：发布时间 1月25日10:00（同一天）
          category: us_europe（美欧板块）
          title_original: "BOJ Keeps Rate Unchanged"（相同标题）
          url: 相同URL
```

**处理逻辑**：

```
步骤1：按发布时间过滤
  筛选 [1月25日18:00, 1月26日18:00] 范围内的新闻

步骤2：按板块分组
  得到 asia_pacific: [新闻X, ...]
      us_europe: [新闻Y, ...]
      domestic: [...]

步骤3：板块内部去重
  在 asia_pacific 板块内去重：新闻X ✅
  在 us_europe 板块内去重：新闻Y ✅

步骤4：跨板块去重（新增）⭐
  对所有板块的新闻进行全局去重
  检查 title_original + url

  新闻X和新闻Y：
    title_original 相同
    url 相同
    → 判定为同一条新闻

  处理：
    保留第一次出现的板块
    删除后续板块中的重复

  假设 asia_pacific 先处理：
    保留 新闻X 在 asia_pacific
    删除 新闻Y 在 us_europe ❌
```

**去重结果**：
```
 asia_pacific: [新闻X, 其他9条] → 10条
 us_europe: [其他10条] → 10条（新闻Y被删除）
 domestic: [其他10条] → 10条

总计：30条（没有跨板块重复）
```

---

##### 💡 为什么选择时效性过滤？

**优势**：

1. **✅ 逻辑简单**
   - 不需要维护显示历史
   - 不需要复杂的去重算法
   - 只需要比较发布时间

2. **✅ 自动过期**
   - 旧新闻自动过期
   - 不需要手动清理
   - 每天都是新鲜内容

3. **✅ 性能好**
   - 时间范围查询（索引优化）
   - 不需要关联历史表
   - 查询速度快

4. **✅ 符合直觉**
   - 用户看"今天的新闻"
   - 昨天的新闻在"昨天的页面"
   - 想看昨天，点击昨天的HTML链接

**实现复杂度**：
```
简单方案（时效性过滤）：
  只需要：WHERE publish_time >= NOW() - 24小时

复杂方案（显示历史）：
  需要：显示历史表 + 关联查询 + 统计计数
```

---

##### 📁 HTML文件命名

**命名规则**：
```
文件名格式：YYYY-MM-DD.html
示例：2026-01-26.html

文件路径：
  public/2026-01-25.html
  public/2026-01-26.html
  public/2026-01-27.html
  ...
```

**访问方式**：
```
最新：https://your-domain.com/2026-01-26.html
昨天：https://your-domain.com/2026-01-25.html
前天：https://your-domain.com/2026-01-24.html
```

**首页跳转**：
```
index.html → 自动跳转到最新的HTML文件
功能：
  1. 检测public目录下最新的HTML文件
  2. 自动重定向到最新文件

示例：
  访问 https://your-domain.com/
  → 自动跳转到 https://your-domain.com/2026-01-26.html
```

---

##### 🗂️ 数据流程图

```
每天定时任务（18:00）：
  ↓
1. 抓取原始数据（156条）
  → 保存到 raw_articles 临时表
  ↓
2. 时效性过滤
  → 选择24小时内的新闻（假设80条）
  ↓
3. 按板块分组
  → asia_pacific: 30条
  → us_europe: 30条
  → domestic: 20条
  ↓
4. 板块内部去重
  → asia_pacific: 30条 → 26条
  → us_europe: 30条 → 27条
  → domestic: 20条 → 20条
  ↓
5. 关键词筛选TOP 10
  → 每个板块筛选10条
  → 总计30条精选新闻
  ↓
6. 全局去重（跨板块）⭐
  → 检查是否有 title_original + url 相同的新闻
  → 如果有，删除后续板块中的重复
  → 假设删除2条重复
  → 总计28条
  ↓
7. 翻译英文新闻
  → 翻译约18条（中文10条除外）
  ↓
8. 生成AI评论
  → 为28条生成评论
  ↓
9. 保存到正式表
  → news_articles 表（新增28条）
  ↓
10. 生成HTML
  → 使用这28条新闻
  → 生成 2026-01-26.html
  → 保存到 public/ 目录
```

---

##### 📊 HTML内容结构

**页面结构**：
```
2026-01-26.html

├─ Header（头部）
│  ├─ 日期：2026年1月26日
│  ├─ 副标题：每日财经新闻
│  └─ 导航：昨天 | 前天 | 历史归档
│
├─ Market Ticker（市场行情）
│  ├─ 上证指数：3,258.76 ▲ +0.31%
│  ├─ 纳斯达克：19,432.50 ▲ +0.82%
│  └─ ...
│
├─ 板块1：国内金融（10条）
│  ├─ 新闻1（标题 + AI锐评）
│  ├─ 新闻2
│  └─ ...
│
├─ 板块2：亚太日本（10条）
│  ├─ 新闻1
│  └─ ...
│
├─ 板块3：美国欧洲（10条）
│  ├─ 新闻1
│  └─ ...
│
└─ Footer（底部）
   ├─ 生成时间：2026-01-26 18:30:00
   └─ 版权信息
```

**导航链接**：
```html
<a href="2026-01-25.html">昨天 → 1月25日</a>
<a href="2026-01-24.html">前天 → 1月24日</a>
<a href="archive/">历史归档 → 全部文件</a>
```

---

##### ⚙️ 配置参数

**可配置参数**：

```python
# 时效性配置
TIME_WINDOW_HOURS = 24        # 时间窗口（小时）
GENERATION_TIME = "18:00"     # 每天生成时间

# 去重配置
ENABLE_CROSS_CATEGORY_DEDUPE = True  # 是否启用跨板块去重

# HTML配置
OUTPUT_FORMAT = "{date}.html"   # 文件名格式
OUTPUT_DIR = "public/"          # 输出目录
```

---

##### 🤔 特殊情况处理

**情况1：某天新闻特别少**

```
场景：节假日，新闻很少
筛选后只有15条（期望30条）

处理：
  放宽时间窗口？
  → 不！保持24小时窗口

  显示不足10条的板块？
  → 是的，如实显示
  → 例如：asia_pacific 只有6条
  → 这6条都是24小时内最新、最重要的
```

**原则**：
- ✅ 宁缺毋滥
- ✅ 显示的都是真实新闻
- ✅ 不会为了凑数而显示旧新闻

---

**情况2：新闻超过预期**

```
场景：重大事件日（如央行降息）
筛选后有50条（期望30条）

处理：
  严格执行TOP 10筛选
  → 每个板块最多10条
  → 总计30条
  → 多余的新闻不显示
```

**原则**：
- ✅ 只显示最精选的30条
- ✅ 保证质量和可读性
- ✅ 不会被新闻淹没

---

**情况3：跨板块重复新闻**

```
场景：同一条新闻应该属于哪个板块？

新闻："日本央行维持利率不变"
  → 既可能是亚太板块
  → 也可能影响美欧市场

处理：
  优先级规则：
  1. 按配置的板块分类（sources.yaml中的category）
  2. 如果配置为 asia_pacific，归入亚太板块
  3. 如果同时配置为多个板块
     → 保留第一次出现的板块
     → 删除后续板块中的重复
```

---

##### ✅ HTML生成需求总结

**核心逻辑（已确认）**：

1. ✅ **时效性过滤（24小时）**
   - 只显示24小时内发布的新闻
   - 自动过期旧新闻
   - 不需要维护显示历史

2. ✅ **跨板块去重**
   - 同一条新闻（title_original + url）
   - 在多个板块中出现
   - 只保留第一个板块
   - 删除后续板块的重复

3. ✅ **每天独立HTML文件**
   - 每天生成一个独立的HTML文件
   - 命名格式：YYYY-MM-DD.html
   - 各天新闻互不重叠

4. ✅ **最新文件自动跳转**
   - 访问根路径自动跳转到最新HTML
   - 可以通过导航访问历史文件

**预期效果**：
```
1月25日：2026-01-25.html（显示1月24-25日的新闻）
1月26日：2026-01-26.html（显示1月25-26日的新闻）
1月27日：2026-01-27.html（显示1月26-27日的新闻）

新闻不重复，每天都是新鲜内容
```

**状态**：✅ 逻辑已确认，可以实施

---

##### 📐 技术实现要点

**数据库查询**：
```sql
-- 时效性过滤查询
SELECT * FROM raw_articles
WHERE publish_time >= datetime('now', '-24 hours')
ORDER BY publish_time DESC;
```

**跨板块去重算法**：
```
伪代码：
1. 按板块筛选后的新闻列表
   all_news = [asia_pacific_news, us_europe_news, domestic_news]

2. 扁平化成一个列表
   flat_list = []
   for category_news in all_news:
       flat_list.extend(category_news)

3. 全局去重
   seen = {}  # {title_original + url: article}
   unique_list = []

   for article in flat_list:
       key = article['title_original'] + article['url']
       if key not in seen:
           seen[key] = article
           unique_list.append(article)
       else:
           logger.info(f"跨板块去重: {article['title']}")

4. 重新分组
   按category重新分组unique_list

5. 如果某板块不足10条
   不补充，如实显示
```

**性能优化**：
```
索引优化：
  CREATE INDEX idx_publish_time ON raw_articles(publish_time DESC);
  CREATE INDEX idx_title_url ON raw_articles(title_original, url);

查询优化：
  只查询24小时内的数据
  → 减少数据量
  → 提高查询速度
```

---
def generate_html():
    """生成 HTML"""
    # 从正式表读取数据
    all_articles = db_manager.get_all_articles()

    # 按板块分组
    by_category = group_by_category(all_articles)

    # 生成 HTML
    generator = HTMLGenerator()
    output_path = generator.generate(by_category)

    logger.info(f"✓ HTML 已生成: {output_path}")
```

**输出**：
- 文件：`public/2026-01-26.html`
- 包含 30 条精选新闻（每板块 10 条）
- 包含中文翻译和 AI 评论

---

## 📁 文件结构

```
news_bot/
├── test_raw_fetch.py              ✅ 阶段1：原始数据抓取（已实现）
├── process_raw_articles.py        ⏳ 阶段2：数据处理脚本（待创建）
├── src/
│   ├── database.py                ✅ 已添加临时表方法
│   ├── scraper.py                 ✅ 抓取器
│   ├── translator.py              ✅ 翻译模块
│   ├── ai_comment.py              ✅ AI 评论生成
│   └── html_generator.py          ✅ HTML 生成
├── data/
│   └── news.db
│       ├── raw_articles           ✅ 临时表（156条）
│       └── news_articles          ✅ 正式表（28条旧数据）
└── 数据去重优化需求文档.md         ✅ 本文档
```

---

## 🎯 实施步骤

### Step 1：创建数据处理脚本 ⏳

**文件**：`process_raw_articles.py`

**功能**：
```python
1. 从 raw_articles 读取数据
2. 按板块分组
3. 每个板块内部去重
4. 根据关键词优先级筛选 TOP 10（高:中:低 = 5:3:2）
5. 翻译英文新闻
6. 生成 AI 评论
7. 保存到 news_articles
8. 生成 HTML
```

---

### Step 2：测试完整流程 ⏳

```bash
# 1. 抓取原始数据
python3 test_raw_fetch.py
# 结果：156 条保存到 raw_articles

# 2. 处理数据
python3 process_raw_articles.py
# 结果：30 条保存到 news_articles + 生成 HTML
```

---

### Step 3：更新 main.py ⏳

将主程序改为两阶段执行：

```python
def main():
    # 阶段1：抓取原始数据
    logger.info("阶段1: 抓取原始数据")
    fetch_and_save_raw()

    # 阶段2：处理数据
    logger.info("阶段2: 处理数据")
    process_raw_articles()
```

---

## 📊 预期效果

### 数据量对比

**之前（有问题的逻辑）**：
```
抓取：156 条
保存：28 条
丢失：128 条 (82%)
```

**优化后**：
```
阶段1：抓取 156 条 → 保存到 raw_articles (156条) ✅
阶段2：
  - asia_pacific: 68条 → 去重后 ~64条 → 按关键词优先级筛选 → TOP 10
  - us_europe: 68条 → 去重后 ~64条 → 按关键词优先级筛选 → TOP 10
  - domestic: 20条 → 去重后 ~20条 → 按关键词优先级筛选 → TOP 10
  最终：30 条精选新闻（每板块10条）
```

### 优势

✅ **数据不丢失**
- 所有 156 条都保存到临时表
- 可以随时查看原始数据

✅ **去重更合理**
- 按板块内部去重
- 不同板块保留各自的数据

✅ **成本可控**
- 只对精选的 30 条进行翻译和 AI 评论
- API 调用：~50 次（翻译20次 + AI评论30次 = ~$0.20）
- 每天运行一次，月成本约 $6（~45元人民币）

✅ **流程清晰**
- 阶段1：数据抓取（可重复运行）
- 阶段2：数据处理（可单独运行）

---

## 🔄 运行方式

### 方式1：分步执行（调试阶段）

```bash
# 步骤1：抓取原始数据
python3 test_raw_fetch.py

# 步骤2：处理数据
python3 process_raw_articles.py
```

### 方式2：一键执行（生产环境）

```bash
# 完整流程
python3 main.py
```

---

## ⚠️ 注意事项

### 1. 临时表清理

**问题**：每次运行是否清空临时表？

**建议**：
- ✅ 阶段1 每次清空（保证数据最新）
- ✅ 阶段2 不清空（可以重复处理）

### 2. 正式表去重

**问题**：news_articles 的唯一约束

**现状**：
- `CONSTRAINT unique_title UNIQUE (title, title_original)`
- 会阻止保存重复新闻

**建议**：
- ✅ 保持现状（避免真正重复的新闻）
- ✅ 不同板块的同一条新闻会被认为是重复（这是合理的）

### 3. 新闻源配置

**问题**：是否修改 sources.yaml？

**建议**：
- ❌ 不修改配置（保持灵活）
- ✅ 通过去重逻辑解决问题
- ✅ 如果需要同一新闻出现在多个板块，可以在后续版本中支持

---

## 📝 后续优化方向

### v2.1（高优先级优化 - 专家建议）

#### 1. 语义重复检测（Fuzzy Matching）
**专家建议**：
> "当前去重完全依赖 title_original 的字符串匹配。同一新闻在彭博社叫 'Fed may cut rates'，在路透社叫 'Fed signals potential rate cuts'。字符串不匹配，但内容完全一致。"

**实施方案**：
- [ ] 引入 Levenshtein 距离算法
- [ ] 如果两个标题相似度超过 85%，且属于同一时间段，则视为重复
- [ ] 相似度阈值可配置（默认 0.85）

**预期效果**：
```
Bloomberg: "Fed may cut rates"
Reuters:   "Fed signals potential rate cuts"
相似度: 87%
结果: 判定为重复，只保留一个
```

#### 2. 引入 SimHash（内容指纹）
**专家建议**：
> "对于 content_original（长内容），建议计算一个 SimHash 存储在 raw_articles 中。这样即使标题稍微改动，只要内容正文 90% 相同，也能实现极高精度的去重。"

**实施方案**：
- [ ] 为 content_original 计算 SimHash
- [ ] 存储在 raw_articles 表中
- [ ] 比较汉明距离（Hamming Distance）
- [ ] 汉明距离 ≤ 3 视为重复

**预期效果**：
```
新闻A: 内容1000字
新闻B: 内容980字，其中90%相同
SimHash汉明距离: 2
结果: 判定为重复
```

#### 3. 错误分级重试
**专家建议**：
> "在翻译失败的状态中，区分 429（频率限制）和 401/403（密钥错误）。429 等待 60 秒自动重试，401 立即停止所有翻译任务并向你发送报警。"

**实施方案**：
- [ ] 区分错误类型（429, 401, 403, 500等）
- [ ] 429错误：等待60秒自动重试
- [ ] 401/403错误：立即停止并报警
- [ ] 500错误：重试3次后放弃
- [ ] 发送通知（邮件/Webhook）

#### 4. 关键词负面列表
**专家建议**：
> "设置关键词得分上限，或者引入负面关键词（如'广告'、'声明'、'免责'）来扣分。"

**实施方案**：
- [ ] 建立负面关键词列表（广告、声明、免责、推广等）
- [ ] 匹配到负面关键词扣分（-20分）
- [ ] 防止SEO垃圾新闻高分

---

### v2.2（中优先级优化）

#### 1. 增强HTML生成
**专家建议**：
> "在 HTML 页面顶部增加一个'过去 3 小时最热'或'今日最重磅'的置顶区域。"

**实施方案**：
- [ ] 添加"今日重磅"板块（score > 100的新闻）
- [ ] 添加"过去3小时最热"板块（最新高分新闻）
- [ ] 优化导航和UI

#### 2. 预览与手动干预
**专家建议**：
> "提供一个极简的后台（甚至是一个 JSON 文件），让你能手动勾选掉那些虽然分高但其实没意义的新闻。"

**实施方案**：
- [ ] 生成HTML前保存预览文件（JSON格式）
- [ ] 提供简单的Web界面或CLI工具
- [ ] 支持手动排除/添加新闻
- [ ] 支持编辑AI评论

---

### v3.0（远期规划）

#### 1. Web管理界面
- [ ] 完整的后台管理系统
- [ ] 新闻源管理
- [ ] 手动编辑新闻
- [ ] 历史数据查看

#### 2. 高级去重算法
- [ ] 基于AI的语义相似度检测
- [ ] 使用Embedding模型
- [ ] 自动识别重复报道

#### 3. 智能推荐
- [ ] 基于用户阅读习惯推荐
- [ ] 个性化新闻排序
- [ ] 用户评分系统

---

## 🎯 版本规划总结

| 版本 | 主要改进 | 实施优先级 | 预计工作量 |
|------|---------|-----------|-----------|
| **v2.0** | 跨板块去重优化、得分上限、事务、时间保底 | P0（必须） | 2-3天 |
| **v2.1** | 语义去重、SimHash、错误分级、负面关键词 | P1（高） | 5-7天 |
| **v2.2** | HTML增强、手动干预 | P2（中） | 3-5天 |
| **v3.0** | Web界面、AI去重、智能推荐 | P3（远期） | 2-4周 |

---

## ✅ 检查清单

实施前：
- [ ] 阅读完整需求文档
- [ ] 理解数据流程
- [ ] 确认 API key 配置

实施后：
- [ ] 测试完整流程
- [ ] 验证数据量（28-30条，每个板块10条减去跨板块重复）
- [ ] 检查 HTML 生成
- [ ] 计算 API 成本

---

## 📞 支持

如有问题，请参考：
- `test_raw_fetch.py` - 阶段1 实现示例
- `src/database.py` - 数据库方法
- `logs/` - 运行日志

---

**文档版本**：v2.1（根据专家评审意见更新）
**最后更新**：2026-01-26
**状态**：待实施 ✍️

**v2.1 改进内容**：
1. ✅ 修改跨板块去重顺序（先去重，再筛选）
2. ✅ 增加候补补齐机制（确保每个板块10条）
3. ✅ 增加关键词得分上限（防止SEO堆砌）
4. ✅ 增加数据库事务说明（BEGIN IMMEDIATE）
5. ✅ 增加publish_time保底逻辑
6. ✅ 将高级优化（语义去重、SimHash等）标记为v2.1
7. ✅ 新增版本规划总结表格
