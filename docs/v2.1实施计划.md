# v2.1需求改进实施计划

## 📋 项目信息

- **创建时间**：2026-01-26
- **需求文档**：数据去重优化需求文档 v2.1
- **分支名称**：feature/实施v2.1需求改进
- **优先级**：P0（必须实施）
- **预计工期**：2-3天

---

## 🎯 实施目标

根据专家评审意见，实施数据去重优化的5大核心改进：

1. ✅ 跨板块去重逻辑优化（核心改进）
2. ✅ 关键词得分上限（防止SEO堆砌）
3. ✅ 数据库事务保证（并发安全）
4. ✅ 时间保底逻辑（防止丢失新闻）
5. ✅ 完整的测试和验证

---

## 📊 任务分解

### 阶段1：准备工作（Day 1 上午）

#### 1.1 环境检查
- [ ] 确认Python版本（3.10+）
- [ ] 确认依赖包已安装
- [ ] 确认数据库文件存在
- [ ] 备份现有数据和代码

#### 1.2 阅读需求文档
- [ ] 重新阅读数据去重优化需求文档 v2.1
- [ ] 理解跨板块去重逻辑
- [ ] 理解关键词评分规则
- [ ] 理解数据库事务要求

---

### 阶段2：核心功能实施（Day 1 下午 - Day 2）

#### 2.1 时间保底逻辑（优先级：最高）⭐

**文件**：`news_bot/src/utils.py`（新建）

**功能**：
```python
# 1. get_effective_publish_time()
#    - 优先使用publish_time
#    - 缺失时使用crawl_time
#    - 记录日志

# 2. parse_timestamp()
#    - 支持多种时间格式
#    - RFC 3339, ISO 8601, RSS格式

# 3. normalize_to_utc()
#    - 统一时区到UTC
#    - 避免时区混淆
```

**测试**：
- [ ] 测试正常时间格式
- [ ] 测试缺失publish_time
- [ ] 测试错误格式
- [ ] 测试时区转换

**预计时间**：2-3小时

---

#### 2.2 跨板块去重逻辑（核心功能）⭐⭐⭐

**文件**：`news_bot/src/deduplicator.py`（新建）

**功能**：
```python
# 1. deduplicate_by_category()
#    - 板块内部去重（保留最新）
#    - 基于 title_original/title

# 2. cross_category_deduplicate()
#    - 跨板块全局去重
#    - 基于 (title_original, url)
#    - 返回去重后的新闻和候补新闻

# 3. fill_top_10()
#    - 填充每个板块到10条
#    - 从候补新闻中补齐
#    - 按关键词评分排序
```

**测试**：
- [ ] 测试板块内去重
- [ ] 测试跨板块去重
- [ ] 测试候补补齐
- [ ] 测试优先级规则

**预计时间**：4-5小时

---

#### 2.3 关键词评分优化

**文件**：`news_bot/src/scorer.py`（修改现有代码）

**功能**：
```python
# 修改 calculate_priority_score()
# - 标题匹配：最多2个关键词，上限+40分
# - 内容匹配：最多1个关键词，上限+10分
# - 总分上限：60分
# - 防止SEO堆砌
```

**测试**：
- [ ] 测试正常关键词匹配
- [ ] 测试SEO堆砌情况
- [ ] 测试得分上限
- [ ] 对比v2.0评分结果

**预计时间**：2小时

---

#### 2.4 数据库事务优化

**文件**：`news_bot/src/database.py`（修改）

**功能**：
```python
# 1. translate_with_lock()
#    - 使用 BEGIN IMMEDIATE 事务
#    - 原子性地获取锁
#    - 异常时自动释放

# 2. generate_comment_with_lock()
#    - 同样的事务机制
#    - 用于AI评论生成

# 3. set_translating_lock()
#    - UPDATE with WHERE translating = 0
#    - 返回是否成功
```

**测试**：
- [ ] 测试正常翻译流程
- [ ] 测试并发翻译（模拟）
- [ ] 测试异常处理
- [ ] 测试锁释放

**预计时间**：3小时

---

### 阶段3：集成和测试（Day 3）

#### 3.1 创建主处理脚本

**文件**：`news_bot/process_raw_articles.py`（新建）

**功能**：
```python
def main():
    """主处理流程"""
    # 1. 从raw_articles读取数据
    # 2. 时间字段处理
    # 3. 按板块分组
    # 4. 板块内去重
    # 5. 跨板块去重
    # 6. 候补补齐
    # 7. 关键词筛选TOP 10
    # 8. 翻译（使用事务）
    # 9. AI评论（使用事务）
    # 10. 保存到news_articles
    # 11. 生成HTML
```

**预计时间**：3-4小时

---

#### 3.2 端到端测试

**测试场景**：
- [ ] 使用真实数据测试完整流程
- [ ] 验证数据量（28-30条）
- [ ] 验证去重效果
- [ ] 验证候补补齐
- [ ] 验证API调用次数
- [ ] 验证成本估算

**预计时间**：2小时

---

#### 3.3 性能测试

**测试项目**：
- [ ] 处理时间（目标：<5分钟）
- [ ] 内存使用
- [ ] 数据库查询效率
- [ ] 并发安全性

**预计时间**：1小时

---

### 阶段4：文档和部署（Day 3 下午）

#### 4.1 更新文档
- [ ] 更新README.md
- [ ] 更新API文档
- [ ] 编写使用指南
- [ ] 记录测试结果

#### 4.2 代码审查准备
- [ ] 添加注释
- [ ] 代码格式化
- [ ] 检查错误处理
- [ ] 准备PR描述

---

## 📁 文件结构

```
news_bot/
├── process_raw_articles.py        # ⭐ 新建：主处理脚本
├── src/
│   ├── utils.py                   # ⭐ 新建：工具函数（时间处理）
│   ├── deduplicator.py            # ⭐ 新建：去重逻辑
│   ├── scorer.py                  # ✏️ 修改：评分优化
│   ├── database.py                # ✏️ 修改：事务优化
│   ├── translator.py              # ✏️ 修改：使用新的事务
│   ├── ai_comment.py              # ✏️ 修改：使用新的事务
│   └── html_generator.py          # 保持不变
├── tests/
│   ├── test_utils.py              # ⭐ 新建：测试时间处理
│   ├── test_deduplicator.py       # ⭐ 新建：测试去重
│   ├── test_scorer.py             # ⭐ 新建：测试评分
│   └── test_integration.py        # ⭐ 新建：集成测试
└── docs/
    └── v2.1实施计划.md             # 本文档
```

---

## 🔧 技术要点

### 1. 时间处理
```python
# 关键函数
def get_effective_publish_time(article) -> datetime:
    """获取有效的发布时间，保底使用crawl_time"""

def parse_timestamp(time_str: str) -> datetime:
    """解析多种时间格式"""

def normalize_to_utc(time_obj: datetime) -> datetime:
    """归一化到UTC时区"""
```

### 2. 去重逻辑
```python
# 两阶段去重
deduped, backup = cross_category_deduplicate(grouped_articles)
final_articles = fill_top_10(deduped, backup)

# 关键：候补补齐机制
if len(articles) < 10:
    needed = 10 - len(articles)
    articles.extend(backup[:needed])
```

### 3. 事务处理
```python
# BEGIN IMMEDIATE 事务
db.execute("BEGIN IMMEDIATE")
try:
    # 原子操作
    success = set_lock(article_id)
    if not success:
        db.execute("ROLLBACK")
        return None

    # 执行操作
    result = translate(article)

    # 提交
    db.execute("COMMIT")
except Exception as e:
    db.execute("ROLLBACK")
    raise
```

### 4. 评分上限
```python
# 关键词得分上限
title_high_count = 0
for keyword in high_keywords:
    if keyword in title:
        score += 20
        title_high_count += 1
        if title_high_count >= 2:  # 最多2个
            break

# 总分上限
return min(score, 60)
```

---

## 📊 成功标准

### 功能性
- ✅ 每个板块都有10条新闻（除非总数不足）
- ✅ 跨板块重复新闻被正确去重
- ✅ 候补补齐机制正常工作
- ✅ 时间缺失的新闻不会丢失
- ✅ 关键词得分有上限

### 性能
- ✅ 处理时间 < 5分钟（156条新闻）
- ✅ API调用次数符合预期
- ✅ 内存使用合理

### 质量保证
- ✅ 所有单元测试通过
- ✅ 集成测试通过
- ✅ 代码有充分的注释
- ✅ 错误处理完善

---

## 🚨 风险和缓解

### 风险1：时间解析失败
**缓解**：
- 支持多种时间格式
- 使用crawl_time作为保底
- 详细日志记录

### 风险2：去重逻辑错误
**缓解**：
- 充分的单元测试
- 使用真实数据验证
- 逐步实施，逐步测试

### 风险3：事务死锁
**缓解**：
- 使用BEGIN IMMEDIATE
- 设置锁超时
- 异常处理

### 风险4：性能问题
**缓解**：
- 性能测试
- 数据库索引优化
- 批量处理

---

## 📅 时间表

| 日期 | 上午 | 下午 |
|------|------|------|
| **Day 1** | 环境检查<br>阅读需求 | 时间保底逻辑<br>跨板块去重（开始） |
| **Day 2** | 跨板块去重（完成）<br>关键词评分 | 数据库事务<br>主处理脚本 |
| **Day 3** | 集成测试<br>性能测试 | 文档更新<br>代码审查<br>准备PR |

---

## ✅ 检查清单

### 开始前
- [ ] 阅读完需求文档v2.1
- [ ] 理解所有改进点
- [ ] 环境准备就绪
- [ ] 备份现有代码

### 实施中
- [ ] 每个功能都写单元测试
- [ ] 提交前测试通过
- [ ] 代码有充分注释
- [ ] Git提交信息清晰

### 完成后
- [ ] 所有测试通过
- [ ] 文档更新完整
- [ ] PR描述清晰
- [ ] 代码审查通过

---

## 📞 支持

如有问题，参考：
- 需求文档：`docs/数据去重优化需求文档.md`
- 测试报告：`docs/需求文档检查报告.md`
- 原有代码：`news_bot/src/`

---

**文档版本**：v1.0
**创建时间**：2026-01-26
**状态**：准备开始实施 ✍️
